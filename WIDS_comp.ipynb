{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    <H1> Introduction </H1>\n",
    "<p>\n",
    "    This notebook contains the steps I took to analyze and label the WiDS 2018 Datathon data. The data contained demographic and behavioral information from a representative sample of survey respondents from India and their usage of traditional financial and mobile financial services. The dataset is a product of InterMedia’s research to help the world’s poorest people take advantage of widely available mobile phones and other digital technology to access financial tools and participate more fully in their local economies. \n",
    "    To obtain the data contact Intermedia directly at http://finclusion@intermedia.org and fill out a data request form [here](http://finclusion.org/data_fiinder/)\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "\n",
    "The goal of this datathon was to determine if a survey respondent was male or female (0 or 1), based on how they answered questions.\n",
    "\n",
    "I performed the following steps to produce a model with a resulting accuracy of 0.96921 when applied to the test data, placing 73/231.\n",
    "\n",
    "    <li>Wrangling the data</li>\n",
    "    <li>Feature selection</li>\n",
    "    <li>Optimization of an XGBoost model for predicting labels on a test dataset</li>\n",
    "    \n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aregel\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "# general imports\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.style\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "mpl.style.use('seaborn-deep')# load data into pandas dataframe\n",
    "\n",
    "#imports for chi-squared\n",
    "from scipy.stats import chi2_contingency\n",
    "from collections import defaultdict\n",
    "\n",
    "# imports for xgboost\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import DMatrix\n",
    "from xgboost import cv\n",
    "from sklearn.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    <H1> Data Wrangle </H1>\n",
    "<p>\n",
    "    In order to build an effective model the data needs to be cleaned and orgainized. \n",
    "    \n",
    "</p>\n",
    "\n",
    "<p>\n",
    "    <li>Read data into memory as a pandas dataframe</li>\n",
    "    <li>Remove empty columns</li>\n",
    "    <li>Insure feature agreement between test and training data</li>\n",
    "    <li>Separate the different data types and cast the categorical data as object type</li>\n",
    "    \n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import training data\n",
    "df_train = pd.read_csv(r'train.csv', low_memory=False)\n",
    "df_test = pd.read_csv(r'test.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all columns with no data (there are no rows with all NaNs)\n",
    "df_nona_tr = df_train.dropna(axis=1, how='all')\n",
    "df_nona_ts = df_test.dropna(axis=1, how='all')\n",
    "# create list of columns both datasets have\n",
    "comb_cols = list(set(df_nona_tr) & set(df_nona_ts))\n",
    "# define cleaned datasets\n",
    "clean_train = df_train[comb_cols]\n",
    "clean_test = df_test[comb_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Separate different data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isolate text data\n",
    "text_train = clean_train.drop(columns=['LN2_RIndLngBEOth','LN2_WIndLngBEOth']).select_dtypes(exclude=['float64','int64'])\n",
    "text_test = clean_test.drop(columns=['LN2_RIndLngBEOth','LN2_WIndLngBEOth']).select_dtypes(exclude=['float64', 'int64'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data dictionary provided a description of all of the catagorical data, so I will use those column names to separate the catagorical data from the numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of catagorical feature names\n",
    "data_dictionary = pd.read_excel('WiDS data dictionary v2.xlsx')\n",
    "col_list = list(data_dictionary['Column Name'][1:].apply(lambda x: str(x)))\n",
    "col_list.append(['LN2_RIndLngBEOth','LN2_WIndLngBEOth'])\n",
    "categorical_column_names = [name for name in comb_cols if name in col_list]\n",
    "# Cast catagorical data as object datatype\n",
    "categorical_train = clean_train[categorical_column_names].drop(columns='DG1').astype('object')\n",
    "categorical_test = clean_test[categorical_column_names].drop(columns='DG1').astype('object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe of numerical data\n",
    "drop_columns = categorical_column_names + list(text_train)\n",
    "numerical_train = clean_train.drop(columns=drop_columns)\n",
    "numerical_test = clean_test.drop(columns=drop_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify data type separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18255 entries, 0 to 18254\n",
      "Columns: 1168 entries, IFI16_5 to MM42_3\n",
      "dtypes: float64(844), int64(237), object(87)\n",
      "memory usage: 162.7+ MB\n",
      "None\n",
      "1167\n"
     ]
    }
   ],
   "source": [
    "print(clean_train.info())\n",
    "print(len(list(numerical_train)) + len(list(categorical_train)) + len(list(text_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<H3> Conclusions for Data Wrangle </H3>\n",
    "\n",
    "<p>\n",
    "The separate columns sum to the total number of columns -1, which makes sense as the labeled column is not included in any of the catagories\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    <H1> Feature Selection </H1>\n",
    "\n",
    "<p>\n",
    "    <li>Remove any column that is more than 60% NaN</li>\n",
    "    <li>Use the Chi Squared metric to determine if the categorical data is dependent on gender</li>\n",
    "    <li>One-hot encode the categorical data</li>\n",
    " \n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform dataframes where NaN = 0 and value=1, and sum them\n",
    "text_count = text_train.notna().astype(int).sum()\n",
    "categorical_count = categorical_train.notna().astype(int).sum()\n",
    "numerical_count = numerical_train.notna().astype(int).sum()\n",
    "# Define threshold for 40% filled in\n",
    "threshold = 18255*0.4\n",
    "\n",
    "# Create list of columns that exceed the threshold\n",
    "valid_text_columns = []\n",
    "valid_categorical_columns = []\n",
    "valid_numerical_columns = []\n",
    "for text_name, text_num, categorical_name, categorical_num,numerical_name, numerical_num in zip(text_count.index, text_count,categorical_count.index,categorical_count,numerical_count.index, numerical_count):\n",
    "    if text_num > threshold:\n",
    "        valid_text_columns.append(text_name)\n",
    "    if categorical_num > threshold:\n",
    "        valid_categorical_columns.append(text_name)\n",
    "    if numerical_num > threshold:\n",
    "        valid_numerical_columns.append(text_name)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "14\n",
      "13\n"
     ]
    }
   ],
   "source": [
    "# print the number of valid columns for each datatype\n",
    "print(len(valid_text_columns))\n",
    "print(len(valid_categorical_columns))\n",
    "print(len(valid_numerical_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see how many valid columns there are with a threshold of only 10% of rows filled\n",
    "\n",
    "# Transform dataframes where NaN = 0 and value=1, and sum them\n",
    "text_count = text_train.notna().astype(int).sum()\n",
    "categorical_count = categorical_train.notna().astype(int).sum()\n",
    "numerical_count = numerical_train.notna().astype(int).sum()\n",
    "# Define threshold for 10% filled in\n",
    "threshold = 18255*0.1\n",
    "\n",
    "# Create list of columns that exceed the threshold\n",
    "valid_text_columns = []\n",
    "valid_categorical_columns = []\n",
    "valid_numerical_columns = []\n",
    "for text_name, text_num, categorical_name, categorical_num,numerical_name, numerical_num in zip(text_count.index, text_count,categorical_count.index,categorical_count,numerical_count.index, numerical_count):\n",
    "    if text_num > threshold:\n",
    "        valid_text_columns.append(text_name)\n",
    "    if categorical_num > threshold:\n",
    "        valid_categorical_columns.append(text_name)\n",
    "    if numerical_num > threshold:\n",
    "        valid_numerical_columns.append(text_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "21\n",
      "31\n"
     ]
    }
   ],
   "source": [
    "print(len(valid_text_columns))\n",
    "print(len(valid_categorical_columns))\n",
    "print(len(valid_numerical_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<p>\n",
    "<H5> Eliminating almost 98% of all the features will probably lead to underfitting when predicting on new data.  However, the text data is especially incomplete, as none of the columns are even 10% complete. Thus, I will not use any of the text data for training the XGBoost model and will use the chi-squared metric to evaluate the categorical data. </H5>\n",
    "</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter out the categorical data not dependent on gender, using the chi-squared test statistic with contingency tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "# function to count values for each possible category\n",
    "def cat_count(pd_series):\n",
    "    categories = list(set(pd_series))\n",
    "    cat_count = dict.fromkeys(categories, 0)\n",
    "    for cat in pd_series:\n",
    "        cat_count[cat] += 1\n",
    "    return cat_count\n",
    "\n",
    "# function to create joint dist table\n",
    "def joint_dist_table(cat_series):\n",
    "    # split male and female counts, and drops and Nans\n",
    "    F_series = cat_series[categorical_train.is_female == 1].dropna()\n",
    "    M_series = cat_series[categorical_train.is_female == 0].dropna()\n",
    "    # find possible categories for each column and gender\n",
    "    F = cat_count(F_series)\n",
    "    M = cat_count(M_series) \n",
    "    # only use the categories that exist for both genders\n",
    "    keep = set(F) & set(M)\n",
    "    F_new = {k: F[k] for k in keep}\n",
    "    M_new = {k: M[k] for k in keep}\n",
    "    # combine counts in dataframe and make distribution table\n",
    "    dist_table = pd.DataFrame.from_dict(F_new, orient='index')\n",
    "    dist_table[1] = M_new.values()\n",
    "    # format the distribution table\n",
    "    final_dist_table = dist_table.rename(columns={0:'Male',1:'Female'}).transpose()\n",
    "    return final_dist_table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the label to the categorical dataframe\n",
    "categorical_train['is_female'] = df_train.is_female"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine p-value of the chi-squared metric for each categorical column\n",
    "chi_dict = defaultdict(list)\n",
    "for cat_cols in list(categorical_train):\n",
    "    try:\n",
    "        # create joint distribution table\n",
    "        jd_table = joint_dist_table(categorical_train[cat_cols])\n",
    "        chi_test_value, chi_p, degfree, exp_val = chi2_contingency(jd_table)\n",
    "        chi_dict[cat_cols] = [chi_test_value, chi_p, degfree, exp_val]\n",
    "    except ValueError:\n",
    "        chi_dict[cat_cols] = [0,0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter columns according to significance level\n",
    "sig_level = 0.01\n",
    "sig_cols = []\n",
    "for k,v in chi_dict.items():\n",
    "    if v[1] < sig_level:\n",
    "        sig_cols.append(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the label column from the significant features list\n",
    "sig_cols.remove('is_female')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe for only the categorical data dependent on gender\n",
    "significant_categorical_data = df_nona_tr[sig_cols].astype('object') # cast to object type\n",
    "encoded_categorical_data = pd.get_dummies(significant_categorical_data, dummy_na=True) # one-hot encode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2135\n"
     ]
    }
   ],
   "source": [
    "# count how many significant columns are left\n",
    "print(len(encoded_categorical_data.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "    <H1> Optimize XGBoost model </H1>\n",
    "        <p>\n",
    "            <li>Determine if using the categorical, numerical, or a combination of the data improves the model</li>\n",
    "            <li>Use cross-fold validation and gridsearch to optimize hyper-parameters</li>\n",
    "            <li>Validate optimized parameters</li>\n",
    "            <li>Predict label for test data</li>\n",
    "        </p>\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "<H7> I'm using the binary logistic loss function because it returns the probability of the label being 1, which is a requirement of the datathon. I will be optimizing using accuracy as that is the metric used for the datathon.</H7>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use unoptomized xgboost model with numerical, categorical, and a combination of the two "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test, train, split\n",
    "# GridSearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model with all of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process test data\n",
    "    # filter significant columns\n",
    "    # hot-encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict label of test data with optimized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create features(X), and target(y)\n",
    "X = encoded_categorical_data\n",
    "y = df_train['is_female']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data for testing and trainning\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_parameters = {\n",
    "               'max_depth':4,\n",
    "               'learning_rate':0.3,\n",
    "               'min_child_weight':3,\n",
    "               'colsample_bytree':0.85,\n",
    "               'subsample':0.85,\n",
    "               'gamma':0,\n",
    "               'max_delta_step':0,\n",
    "               'colsample_bylevel':1,\n",
    "               'scale_pos_weight':1,\n",
    "               'base_score':0.5,\n",
    "               'seed':5,\n",
    "               'objective':'binary:logistic',\n",
    "               'silent': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_reg = xgb.XGBRegressor(**fixed_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_reg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xg_reg.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [round(value) for value in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(xg_reg, open('xgb_final_model.dat', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_dummy[model_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = xg_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nona_ts.test_id.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = {'test_id': df_nona_ts.test_id.values, 'is_female':y_test}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = pd.DataFrame(df_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.to_csv('sub4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
